{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <Font color = 'indianred'>**Sentiment Analysis using Hugging Face Ecosystem** </font>"
      ],
      "metadata": {
        "id": "WSUdmHrfC1sD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIxAU6f3S-MI"
      },
      "source": [
        "## <Font color = 'indianred'>**1. Set Environment**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# If in Colab, then import the drive module from google.colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google.colab import drive\n",
        "  # Mount the Google Drive to access files stored there\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # Install the latest version of torchtext library quitely without showing output\n",
        "  !pip install torchtext -qq\n",
        "  !pip install transformers evaluate wandb datasets accelerate -U -qq\n",
        "  !pip install transformers evaluate wandb datasets accelerate peft bitsandbytes -U -qq\n",
        "\n",
        "  basepath = '/content/drive/MyDrive/NLP/Projects'\n",
        "\n",
        "else:\n",
        "  basepath = '/content/drive/MyDrive/NLP/Projects'"
      ],
      "metadata": {
        "id": "DV_0TaQT1N8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standard data science librraies for data handling and v isualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# Importing PyTorch library for tensor computations and neural network modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# For working with textual data vocabularies and for displaying model summaries\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "# Load data fille\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from datasets import Dataset\n",
        "\n",
        "# Utilities for efficient serialization/deserialization of Python objects and for element tallying\n",
        "import joblib\n",
        "from collections import Counter\n",
        "\n",
        "# For creating lightweight attribute classes and for partial function application\n",
        "from functools import partial\n",
        "\n",
        "# For filesystem path handling, generating and displaying confusion matrices, and date-time manipulations\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from datetime import datetime\n",
        "\n",
        "# New libraries introduced in this notebook\n",
        "from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score, f1_score\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoConfig\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from transformers import PreTrainedModel, PretrainedConfig\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "import wandb\n",
        "import evaluate\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    LoraConfig,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        ")"
      ],
      "metadata": {
        "id": "G7eJgZYSRBu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Setting up the device for GPU usage\n",
        "\n",
        "# from torch import cuda\n",
        "# device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "qz9JQVYjGtPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "biPubP9gK85Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-24T09:22:06.160646Z",
          "iopub.status.busy": "2022-10-24T09:22:06.160510Z",
          "iopub.status.idle": "2022-10-24T09:22:06.184362Z",
          "shell.execute_reply": "2022-10-24T09:22:06.184015Z",
          "shell.execute_reply.started": "2022-10-24T09:22:06.160633Z"
        },
        "id": "Z0yKILuteTDE"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "\n",
        "base_folder = Path(basepath)\n",
        "data_folder = base_folder/'datasets/aclImdb'\n",
        "model_folder = base_folder/'models/imdb/nn'\n",
        "custom_functions = base_folder/'custom-functions'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC90yjKdZCcB"
      },
      "outputs": [],
      "source": [
        "model_folder.mkdir(exist_ok=True, parents = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_folder"
      ],
      "metadata": {
        "id": "9safg0MgtVtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'indianred'>**2. Load  data**"
      ],
      "metadata": {
        "id": "yr9q1KC3OYF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data from hugging face\n",
        "emotion_data = load_dataset('harpreetmann/train_emotion_spring_2024')"
      ],
      "metadata": {
        "id": "Cxic9X6wx-_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_data['train'][0:2]"
      ],
      "metadata": {
        "id": "tlYkxiJAyaOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['anger',\n",
        " 'anticipation',\n",
        " 'disgust',\n",
        " 'fear',\n",
        " 'joy',\n",
        " 'love',\n",
        " 'optimism',\n",
        " 'pessimism',\n",
        " 'sadness',\n",
        " 'surprise',\n",
        " 'trust']"
      ],
      "metadata": {
        "id": "O7WdwHqUyzfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data = load_dataset('csv', data_files= str(base_folder/'emotion_detection_train.csv'))\n",
        "test_data = load_dataset('csv', data_files= str(base_folder/'emotion_detection_test.csv'))"
      ],
      "metadata": {
        "id": "q-qPM5QdPEpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "SuVT5khL1meu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['train'][0]"
      ],
      "metadata": {
        "id": "OLy1wS-P2237"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels = [label for label in test_data['train'].features.keys() if label not in ['ID', 'Tweet']]\n",
        "# labels"
      ],
      "metadata": {
        "id": "lg0iXYPkJEey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_text = test_data.remove_columns(['ID', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'])\n",
        "test_data_text = test_data_text.rename_column('Tweet', 'text')"
      ],
      "metadata": {
        "id": "Doi6OdZd42U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize counters for each label in train and valid sets\n",
        "train_label_counts = Counter()\n",
        "valid_label_counts = Counter()\n",
        "\n",
        "# Function to update counts\n",
        "def update_label_counts(dataset, label_counts):\n",
        "    for label_array in dataset['label']:\n",
        "        # print(label_array)\n",
        "        # label_array is expected to be a list of binary values\n",
        "        for index, label in enumerate(label_array):\n",
        "            if label == 1:\n",
        "                label_counts[index] += 1\n",
        "\n",
        "# Update counts for both datasets\n",
        "update_label_counts(emotion_data['train'], train_label_counts)\n",
        "update_label_counts(emotion_data['valid'], valid_label_counts)\n",
        "\n",
        "print(train_label_counts)\n",
        "\n",
        "# Display the label distributions\n",
        "print(\"Training set label distribution:\")\n",
        "for label, count in train_label_counts.items():\n",
        "    print(f\"Label {labels[label]}: {count}\")\n",
        "\n",
        "print(\"\\nValidation set label distribution:\")\n",
        "for label, count in valid_label_counts.items():\n",
        "    print(f\"Label {labels[label]}: {count}\")"
      ],
      "metadata": {
        "id": "pweZW3dC1JGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data (assuming you have labels)\n",
        "labels = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust', 'love', 'anticipation', 'optimism']\n",
        "train_counts = [2306, 2330, 1084, 2293, 1850, 714, 306, 656, 891, 1818]\n",
        "valid_counts = [553, 591, 279, 584, 423, 82, 94, 176, 211, 473]\n",
        "\n",
        "x = range(len(labels))  # the label locations\n",
        "\n",
        "# Create two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Training set distribution\n",
        "ax1.bar(x, train_counts, color='skyblue', alpha=0.7, label='Training Set')\n",
        "ax1.set_title('Training Set Label Distribution')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax1.set_xlabel('Labels')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.legend()\n",
        "\n",
        "# Validation set distribution\n",
        "ax2.bar(x, valid_counts, color='coral', alpha=0.7, label='Validation Set')\n",
        "ax2.set_title('Validation Set Label Distribution')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax2.set_xlabel('Labels')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.legend()\n",
        "\n",
        "# Tight layout\n",
        "fig.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1StOboOBRb4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <Font color = 'skyblue'>**Task 1 -part A (LoRA)** </font>"
      ],
      "metadata": {
        "id": "94CFZqHy1Jw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'indianred'>**1. Load pre-trained Tokenizer** </font>"
      ],
      "metadata": {
        "id": "12ZibHDHQZB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# google-gemma\n",
        "checkpoint = \"google/gemma-1.1-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "QpU34DD7Q4W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(batch):\n",
        "    return tokenizer(text = batch[\"text\"], truncation=True)"
      ],
      "metadata": {
        "id": "EpcKEbcQRDf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset= emotion_data.map(tokenize_fn, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(\n",
        "    ['text']\n",
        ")\n",
        "# tokenized_dataset.set_format(type='torch')"
      ],
      "metadata": {
        "id": "Yr_Am7JoRIqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "plkZ9PhWRmP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_dataset = test_data_text.map(tokenize_fn, batched=True)\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(\n",
        "    ['text']\n",
        ")"
      ],
      "metadata": {
        "id": "bCsUM-IV4YWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_dataset"
      ],
      "metadata": {
        "id": "p67mCbtB-___"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color = 'indianred'> **2. Model Training**"
      ],
      "metadata": {
        "id": "T3s7OveLR4kZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **2.1. compute_metrics function** </font>\n"
      ],
      "metadata": {
        "id": "YgzsQWcbR9fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load('accuracy', 'multilabel')\n",
        "f1 = evaluate.load('f1','multilabel')\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # print(logits.shape)\n",
        "    preds = (logits > 0).astype(int)\n",
        "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    f1_micro = f1.compute(predictions=preds, references=labels, average='micro')\n",
        "    f1_macro = f1.compute(predictions=preds, references=labels, average='macro')\n",
        "    return {'f1_micro':f1_micro['f1'],\n",
        "            'f1_macro':f1_macro['f1'],\n",
        "            'accuracy':accuracy['accuracy'],\n",
        "            }"
      ],
      "metadata": {
        "id": "2YJ82_VTSBWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **2.2. Training Arguments** </font>"
      ],
      "metadata": {
        "id": "4rFQzL5DSCny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where model checkpoints will be saved\n",
        "run_name = \"emotions_google_gemma\"\n",
        "base_folder = Path(basepath)\n",
        "model_folder = base_folder / \"models\"/run_name\n",
        "# Create the directory if it doesn't exist\n",
        "model_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Configure training parameters\n",
        "training_args = TrainingArguments(\n",
        "    # Training-specific configurations\n",
        "    num_train_epochs=5,  # Total number of training epochs\n",
        "    # Number of samples per training batch for each device\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "\n",
        "    weight_decay=0.1,  # Apply L2 regularization to prevent overfitting\n",
        "    learning_rate=1e-4,  # Step size for the optimizer during training\n",
        "    lr_scheduler_type='linear',\n",
        "    warmup_steps=0,  # Number of warmup steps for the learning rate scheduler\n",
        "    optim='adamw_torch',  # Optimizer,\n",
        "    max_grad_norm = 1.0,\n",
        "\n",
        "    # Checkpoint saving and model evaluation settings\n",
        "    output_dir=str(model_folder),  # Directory to save model checkpoints\n",
        "    evaluation_strategy='steps',  # Evaluate model at specified step intervals\n",
        "    eval_steps=20,  # Perform evaluation every 10 training steps\n",
        "    save_strategy=\"steps\",  # Save model checkpoint at specified step intervals\n",
        "    save_steps=20,  # Save a model checkpoint every 10 training steps\n",
        "    load_best_model_at_end=True,  # Reload the best model at the end of training\n",
        "    save_total_limit=2,  # Retain only the best and the most recent model checkpoints\n",
        "    # Use 'accuracy' as the metric to determine the best model\n",
        "    metric_for_best_model=\"eval_f1_macro\",\n",
        "    greater_is_better=True,  # A model is 'better' if its accuracy is higher\n",
        "\n",
        "\n",
        "    # Experiment logging configurations (commented out in this example)\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=20,\n",
        "    report_to='wandb',  # Log metrics and results to Weights & Biases platform\n",
        "    run_name=run_name,  # Experiment name for Weights & Biases\n",
        "\n",
        "    fp16=False\n",
        "    # bf16=True\n",
        "    # tf32= False\n",
        ")"
      ],
      "metadata": {
        "id": "u9I9LvNQ1r5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.3. Specify Model** <font/>"
      ],
      "metadata": {
        "id": "VARmxnBJSLl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
        "                                                           num_labels=11,\n",
        "                                                           problem_type=\"multi_label_classification\" )\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "id2label= {id_: label_ for id_, label_ in enumerate(labels)}\n",
        "label2id = {label_: id_ for id_, label_ in enumerate(labels)}\n",
        "config.id2label = id2label\n",
        "config.label2id = label2id\n",
        "model.config = config\n",
        "# model"
      ],
      "metadata": {
        "id": "tzzpmYnqSU8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = model.to(device)"
      ],
      "metadata": {
        "id": "VT1wU3vtJIF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "9x_pefoa3e7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.4. LoRA Setup** <font/>"
      ],
      "metadata": {
        "id": "pXFhsxq038zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import (\n",
        "    TaskType,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        ")"
      ],
      "metadata": {
        "id": "XLVOwkJ06hn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "model_modules = str(model.modules)\n",
        "pattern = r'\\((\\w+)\\): Linear'\n",
        "linear_layer_names = re.findall(pattern, model_modules)\n",
        "\n",
        "names = []\n",
        "# Print the names of the Linear layers\n",
        "for name in linear_layer_names:\n",
        "    names.append(name)\n",
        "target_modules = list(set(names))\n",
        "target_modules"
      ],
      "metadata": {
        "id": "Q_IgZBh44Gb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=128,\n",
        "    lora_alpha=256,\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"lora_only\",\n",
        "    target_modules = ['gate_proj', 'k_proj', 'o_proj', 'q_proj', 'score', 'v_proj'])\n",
        "gemma_model = get_peft_model(model, gemma_config )\n",
        "gemma_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "DEyU3xpJ4Jxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_config.target_modules"
      ],
      "metadata": {
        "id": "RDtynr_p_PZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_model"
      ],
      "metadata": {
        "id": "6lUJBGrB_ZSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92pFNV_e_OvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.5. Custom Trainer**<font/>"
      ],
      "metadata": {
        "id": "gb4eJWpHSstO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pos_weights(dataset):\n",
        "    # Initialize counters for all labels\n",
        "    num_labels = len(dataset['train']['label'][0])\n",
        "    total_positives = [0] * num_labels\n",
        "    total_negatives = [0] * num_labels\n",
        "\n",
        "    # Count positives and negatives for each label\n",
        "    for label_array in dataset['train']['label']:\n",
        "        for i, label in enumerate(label_array):\n",
        "            if label == 1:\n",
        "                total_positives[i] += 1\n",
        "            else:\n",
        "                total_negatives[i] += 1\n",
        "\n",
        "    # Calculate pos_weight for each label\n",
        "    pos_weight = [total_negatives[i] / max(total_positives[i], 1) for i in range(num_labels)]\n",
        "    return torch.tensor(pos_weight)\n",
        "\n",
        "# Calculate the pos_weight using the training set\n",
        "pos_weights = calculate_pos_weights(emotion_data)\n"
      ],
      "metadata": {
        "id": "laxyoU9FSXby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights"
      ],
      "metadata": {
        "id": "-iJaPXrxSY1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights= torch.tensor([2., 3., 2., 2., 2., 3., 2., 3., 2., 4., 4.])"
      ],
      "metadata": {
        "id": "k4eJUBxTSrt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # print(inputs)\n",
        "        # Extract labels and remove them from inputs\n",
        "        labels = inputs.pop(\"labels\").float()  # Ensure labels are float for BCE loss\n",
        "        # print(labels)\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        # Compute custom loss (BCEWithLogitsLoss is suitable for multi-label)\n",
        "        # pos_weight can be used to handle class imbalance\n",
        "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weights.to(device))\n",
        "        # Reshape labels to match logits dimensions\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "wna6C8WXTLXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=gemma_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"valid\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "MAnFdSQlTMSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='indianred'>**2.6. Setup WanDB**<font/>"
      ],
      "metadata": {
        "id": "uQge6j1ETNNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "%env WANDB_PROJECT = emotions_kaggle_S2024"
      ],
      "metadata": {
        "id": "1tnHFjSITUBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'indianred'> **2.7. Start Training** <font/>"
      ],
      "metadata": {
        "id": "eyapcoLaTWQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()  # start training"
      ],
      "metadata": {
        "id": "Dk0leHT8ubTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()  # start training"
      ],
      "metadata": {
        "id": "9bsw6-jDTiCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='indianred'> **2.8. Validation**<font/>"
      ],
      "metadata": {
        "id": "a6qKJaGKTi4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(tokenized_dataset[\"valid\"])"
      ],
      "metadata": {
        "id": "Dk_w-4-JTryB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results"
      ],
      "metadata": {
        "id": "eIDkkgmeTz0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.log({\"eval_accuracy\": eval_results[\"eval_accuracy\"], \"eval_loss\": eval_results[\"eval_loss\"],\n",
        "\"eval_f1_micro\": eval_results[\"eval_f1_micro\"], \"eval_f1_macro\": eval_results[\"eval_f1_macro\"]})"
      ],
      "metadata": {
        "id": "3HRGfbprT05r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **Check Confusion Matrix**</font>"
      ],
      "metadata": {
        "id": "7e_0DHMAT14b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trainer to generate predictions on the tokenized validation dataset.\n",
        "# The resulting object, valid_output, will contain the model's logits (raw prediction scores) for each input in the validation set.\n",
        "valid_output = trainer.predict(tokenized_dataset[\"valid\"])"
      ],
      "metadata": {
        "id": "iaskHVaIT30L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_valid = (valid_output.predictions > 0).astype(int)\n",
        "labels_valid = valid_output.label_ids.astype(int)"
      ],
      "metadata": {
        "id": "dSRshu1ST6Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = labels_valid\n",
        "y_pred = predictions_valid\n",
        "class_names = labels\n",
        "\n",
        "mcm = multilabel_confusion_matrix(y_true, y_pred,)\n",
        "\n",
        "# 1. Individual Heatmaps\n",
        "for idx, matrix in enumerate(mcm):\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues',\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['True Negative', 'True Positive'])\n",
        "    plt.title(f'Confusion Matrix for {class_names[idx]}')\n",
        "    plt.show()\n",
        "\n",
        "# 2. Aggregate Metrics Heatmap\n",
        "precision_per_class = precision_score(y_true, y_pred, average=None)\n",
        "recall_per_class = recall_score(y_true, y_pred, average=None)\n",
        "f1_per_class = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Precision': precision_per_class,\n",
        "    'Recall': recall_per_class,\n",
        "    'F1-Score': f1_per_class\n",
        "}, index=class_names)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(metrics_df, annot=True, cmap='Blues')\n",
        "# plt.title('Metrics for each class')\n",
        "# plt.show()\n",
        "\n",
        "ax = sns.heatmap(metrics_df, annot=True, cmap='Blues')\n",
        "plt.title('Metrics for each class')\n",
        "plt.tight_layout()  # Adjust layout to not cut off edges\n",
        "\n",
        "# Log the heatmap to wandb\n",
        "wandb.log({\"Metrics Heatmap\": wandb.Image(ax.get_figure())})\n",
        "plt.show()\n",
        "\n",
        "# 3. Histogram of Metrics\n",
        "metrics_df.plot(kind='bar', figsize=(12, 7))\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision, Recall, and F1-Score for Each Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uudkQZ98T7X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "hR3XBr2GT9Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###  <font color = 'indianred'> **Save the model on HuggingFace**</font>"
      ],
      "metadata": {
        "id": "_SkaKqgEAYJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"yunaseo/google_gemma_emotion_detection\")"
      ],
      "metadata": {
        "id": "NNmnJjxRqaXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'indianred'>**3. Test Data Prediction** </font>"
      ],
      "metadata": {
        "id": "hYBH3kj6U_aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = trainer.predict(tokenized_test_dataset[\"train\"])"
      ],
      "metadata": {
        "id": "g5XUA40e_OgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing for multi-label classification\n",
        "threshold = 0.5  # Example threshold\n",
        "predicted_labels = (predictions.predictions > threshold).astype(int)\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels = [[label for label, binary in zip(labels, binary_labels) if binary] for binary_labels in predicted_labels]\n",
        "\n",
        "# Print or use the predicted labels\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "id": "yCSk_M65A_hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels[0]"
      ],
      "metadata": {
        "id": "o1MtsC-j0JI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "ojAWrmfMEzRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting data from the 'train' split of test_data\n",
        "test_data_train = test_data['train']\n",
        "tweet_ids = test_data_train['ID']\n",
        "num_tweets = len(tweet_ids)"
      ],
      "metadata": {
        "id": "zjGuthwlHYwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing an empty dictionary to store the predicted labels\n",
        "predicted_labels_dict = {category: [0] * num_tweets for category in ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']}\n",
        "\n",
        "# Iterate over each row of predicted labels and update the dictionary\n",
        "for i, labels in enumerate(predicted_labels):\n",
        "    for label in labels:\n",
        "        predicted_labels_dict[label][i] = 1"
      ],
      "metadata": {
        "id": "uzjo3UBUHbNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(predicted_labels_dict)\n",
        "\n",
        "# Insert tweet IDs as the first column\n",
        "df.insert(0, 'ID', tweet_ids)"
      ],
      "metadata": {
        "id": "QOgF7OLtHfBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Zai6A0WDHg99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('gemma_predicted.csv', index=False)"
      ],
      "metadata": {
        "id": "_RMEzybFHqNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8AB5N1NtlaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('gemma_predicted.csv')"
      ],
      "metadata": {
        "id": "2GF0AUNdIM3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('gemma_predicted.csv')"
      ],
      "metadata": {
        "id": "jJhYnvg-1PLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <Font color = 'skyblue'>**Task 1 -part B (QLoRA)** </font>"
      ],
      "metadata": {
        "id": "Lpwji9aU1Rp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'indianred'>**1. Load pre-trained Tokenizer** </font>"
      ],
      "metadata": {
        "id": "m4GxqANW1RqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# google-gemma\n",
        "checkpoint = \"google/gemma-1.1-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "FFQW5rQS1RqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(batch):\n",
        "    return tokenizer(text = batch[\"text\"], truncation=True)"
      ],
      "metadata": {
        "id": "kC5bE3Ip1RqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset= emotion_data.map(tokenize_fn, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(\n",
        "    ['text']\n",
        ")\n",
        "# tokenized_dataset.set_format(type='torch')"
      ],
      "metadata": {
        "id": "P3T8MJHa1RqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "wQfnYIob1RqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_dataset = test_data_text.map(tokenize_fn, batched=True)\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(\n",
        "    ['text']\n",
        ")"
      ],
      "metadata": {
        "id": "Yo90yQ8H1RqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_dataset"
      ],
      "metadata": {
        "id": "XlxQlCy11RqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color = 'indianred'> **2. Model Training**"
      ],
      "metadata": {
        "id": "9Z7vPjVy1RqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **2.1. compute_metrics function** </font>\n"
      ],
      "metadata": {
        "id": "nIbNImJl1RqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load('accuracy', 'multilabel')\n",
        "f1 = evaluate.load('f1','multilabel')\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # print(logits.shape)\n",
        "    preds = (logits > 0).astype(int)\n",
        "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    f1_micro = f1.compute(predictions=preds, references=labels, average='micro')\n",
        "    f1_macro = f1.compute(predictions=preds, references=labels, average='macro')\n",
        "    return {'f1_micro':f1_micro['f1'],\n",
        "            'f1_macro':f1_macro['f1'],\n",
        "            'accuracy':accuracy['accuracy'],\n",
        "            }"
      ],
      "metadata": {
        "id": "hddRDE7M1RqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **2.2. Training Arguments** </font>"
      ],
      "metadata": {
        "id": "LkoSDk6T1RqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where model checkpoints will be saved\n",
        "run_name = \"emotions_google_gemma\"\n",
        "base_folder = Path(basepath)\n",
        "model_folder = base_folder / \"models\"/run_name\n",
        "# Create the directory if it doesn't exist\n",
        "model_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Configure training parameters\n",
        "training_args = TrainingArguments(\n",
        "    # Training-specific configurations\n",
        "    num_train_epochs=5,  # Total number of training epochs\n",
        "    # Number of samples per training batch for each device\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "\n",
        "    weight_decay=0.1,  # Apply L2 regularization to prevent overfitting\n",
        "    learning_rate=1e-4,  # Step size for the optimizer during training\n",
        "    lr_scheduler_type='linear',\n",
        "    warmup_steps=0,  # Number of warmup steps for the learning rate scheduler\n",
        "    optim='adamw_torch',  # Optimizer,\n",
        "    max_grad_norm = 1.0,\n",
        "\n",
        "    # Checkpoint saving and model evaluation settings\n",
        "    output_dir=str(model_folder),  # Directory to save model checkpoints\n",
        "    evaluation_strategy='steps',  # Evaluate model at specified step intervals\n",
        "    eval_steps=20,  # Perform evaluation every 10 training steps\n",
        "    save_strategy=\"steps\",  # Save model checkpoint at specified step intervals\n",
        "    save_steps=20,  # Save a model checkpoint every 10 training steps\n",
        "    load_best_model_at_end=True,  # Reload the best model at the end of training\n",
        "    save_total_limit=2,  # Retain only the best and the most recent model checkpoints\n",
        "    # Use 'accuracy' as the metric to determine the best model\n",
        "    metric_for_best_model=\"eval_f1_macro\",\n",
        "    greater_is_better=True,  # A model is 'better' if its accuracy is higher\n",
        "\n",
        "\n",
        "    # Experiment logging configurations (commented out in this example)\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=20,\n",
        "    report_to='wandb',  # Log metrics and results to Weights & Biases platform\n",
        "    run_name=run_name,  # Experiment name for Weights & Biases\n",
        "\n",
        "    fp16=False\n",
        "    # bf16=True\n",
        "    # tf32= False\n",
        ")"
      ],
      "metadata": {
        "id": "SN8VS6Rg1RqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pj6mOFEhmDEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.3. Specify Model** <font/>"
      ],
      "metadata": {
        "id": "QAMwlTDU1RqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "1vtydcqU1RqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
        "                                                           num_labels=11,\n",
        "                                                           problem_type=\"multi_label_classification\",\n",
        "                                                           quantization_config=bnb_config)\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "id2label= {id_: label_ for id_, label_ in enumerate(labels)}\n",
        "label2id = {label_: id_ for id_, label_ in enumerate(labels)}\n",
        "config.id2label = id2label\n",
        "config.label2id = label2id\n",
        "model.config = config\n",
        "# model"
      ],
      "metadata": {
        "id": "1YwCP5DEmL1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
        "                                                           num_labels=11,\n",
        "                                                           problem_type=\"multi_label_classification\",\n",
        "                                                           quantization_config=bnb_config)\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "id2label= {id_: label_ for id_, label_ in enumerate(labels)}\n",
        "label2id = {label_: id_ for id_, label_ in enumerate(labels)}\n",
        "config.id2label = id2label\n",
        "config.label2id = label2id\n",
        "model.config = config\n",
        "# model"
      ],
      "metadata": {
        "id": "PvKUoxYl1RqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = model.to(device)"
      ],
      "metadata": {
        "id": "6NN3wDSB1RqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "LXPeNtDz1RqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.4. LoRA Setup** <font/>"
      ],
      "metadata": {
        "id": "6zx0OqyJ1RqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "model_modules = str(model.modules)\n",
        "pattern = r'\\((\\w+)\\): Linear'\n",
        "linear_layer_names = re.findall(pattern, model_modules)\n",
        "\n",
        "names = []\n",
        "# Print the names of the Linear layers\n",
        "for name in linear_layer_names:\n",
        "    names.append(name)\n",
        "target_modules = list(set(names))\n",
        "target_modules"
      ],
      "metadata": {
        "id": "nfdKxFd11RqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=128,\n",
        "    lora_alpha=256,\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"lora_only\",\n",
        "    #modules_to_save = ['score'],\n",
        "    target_modules=target_modules)\n",
        "gemma_model = get_peft_model(model, gemma_config )\n",
        "gemma_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "Ug9SJRYU1RqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_config.target_modules"
      ],
      "metadata": {
        "id": "PuVSnWwW1RqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_model"
      ],
      "metadata": {
        "id": "_9l3XA0o1RqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AvEl-e7V1RqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.5. Custom Trainer**<font/>"
      ],
      "metadata": {
        "id": "pj-iouwP1RqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pos_weights(dataset):\n",
        "    # Initialize counters for all labels\n",
        "    num_labels = len(dataset['train']['label'][0])\n",
        "    total_positives = [0] * num_labels\n",
        "    total_negatives = [0] * num_labels\n",
        "\n",
        "    # Count positives and negatives for each label\n",
        "    for label_array in dataset['train']['label']:\n",
        "        for i, label in enumerate(label_array):\n",
        "            if label == 1:\n",
        "                total_positives[i] += 1\n",
        "            else:\n",
        "                total_negatives[i] += 1\n",
        "\n",
        "    # Calculate pos_weight for each label\n",
        "    pos_weight = [total_negatives[i] / max(total_positives[i], 1) for i in range(num_labels)]\n",
        "    return torch.tensor(pos_weight)\n",
        "\n",
        "# Calculate the pos_weight using the training set\n",
        "pos_weights = calculate_pos_weights(emotion_data)\n"
      ],
      "metadata": {
        "id": "k1zUPPMB1RqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights"
      ],
      "metadata": {
        "id": "XLLlXMLr1RqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights= torch.tensor([2., 3., 2., 2., 2., 3., 2., 3., 2., 4., 4.])"
      ],
      "metadata": {
        "id": "USaG1pX91RqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # print(inputs)\n",
        "        # Extract labels and remove them from inputs\n",
        "        labels = inputs.pop(\"labels\").float()  # Ensure labels are float for BCE loss\n",
        "        # print(labels)\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        # Compute custom loss (BCEWithLogitsLoss is suitable for multi-label)\n",
        "        # pos_weight can be used to handle class imbalance\n",
        "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weights.to(device))\n",
        "        # Reshape labels to match logits dimensions\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "bNzojMYA1RqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=gemma_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"valid\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "OYGbxskP1RqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='indianred'>**2.6. Setup WanDB**<font/>"
      ],
      "metadata": {
        "id": "Zsal8wFl1RqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "%env WANDB_PROJECT = emotions_kaggle_S2024"
      ],
      "metadata": {
        "id": "boRUGl_31RqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'indianred'> **2.7. Start Training** <font/>"
      ],
      "metadata": {
        "id": "N4-nS9FC1RqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()  # start training"
      ],
      "metadata": {
        "id": "WdEewhYxnAyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()  # start training"
      ],
      "metadata": {
        "id": "clRRdFV01RqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='indianred'> **2.8. Validation**<font/>"
      ],
      "metadata": {
        "id": "OjCyJXPg1RqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(tokenized_dataset[\"valid\"])"
      ],
      "metadata": {
        "id": "MNN_ajv1IEPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results"
      ],
      "metadata": {
        "id": "e7-jmil31RqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.log({\"eval_accuracy\": eval_results[\"eval_accuracy\"], \"eval_loss\": eval_results[\"eval_loss\"],\n",
        "\"eval_f1_micro\": eval_results[\"eval_f1_micro\"], \"eval_f1_macro\": eval_results[\"eval_f1_macro\"]})"
      ],
      "metadata": {
        "id": "c8NgBaFK1RqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **Check Confusion Matrix**</font>"
      ],
      "metadata": {
        "id": "lHMIl7cw1RqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trainer to generate predictions on the tokenized validation dataset.\n",
        "# The resulting object, valid_output, will contain the model's logits (raw prediction scores) for each input in the validation set.\n",
        "valid_output = trainer.predict(tokenized_dataset[\"valid\"])"
      ],
      "metadata": {
        "id": "lYIxfpEO1RqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_valid = (valid_output.predictions > 0).astype(int)\n",
        "labels_valid = valid_output.label_ids.astype(int)"
      ],
      "metadata": {
        "id": "ArUxPT821RqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = labels_valid\n",
        "y_pred = predictions_valid\n",
        "class_names = labels\n",
        "\n",
        "mcm = multilabel_confusion_matrix(y_true, y_pred,)\n",
        "\n",
        "# 1. Individual Heatmaps\n",
        "for idx, matrix in enumerate(mcm):\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues',\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['True Negative', 'True Positive'])\n",
        "    plt.title(f'Confusion Matrix for {class_names[idx]}')\n",
        "    plt.show()\n",
        "\n",
        "# 2. Aggregate Metrics Heatmap\n",
        "precision_per_class = precision_score(y_true, y_pred, average=None)\n",
        "recall_per_class = recall_score(y_true, y_pred, average=None)\n",
        "f1_per_class = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Precision': precision_per_class,\n",
        "    'Recall': recall_per_class,\n",
        "    'F1-Score': f1_per_class\n",
        "}, index=class_names)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(metrics_df, annot=True, cmap='Blues')\n",
        "# plt.title('Metrics for each class')\n",
        "# plt.show()\n",
        "\n",
        "ax = sns.heatmap(metrics_df, annot=True, cmap='Blues')\n",
        "plt.title('Metrics for each class')\n",
        "plt.tight_layout()  # Adjust layout to not cut off edges\n",
        "\n",
        "# Log the heatmap to wandb\n",
        "wandb.log({\"Metrics Heatmap\": wandb.Image(ax.get_figure())})\n",
        "plt.show()\n",
        "\n",
        "# 3. Histogram of Metrics\n",
        "metrics_df.plot(kind='bar', figsize=(12, 7))\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision, Recall, and F1-Score for Each Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V-rp-SYA1RqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "P5yTiRHM1RqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###  <font color = 'indianred'> **Save the model on HuggingFace**</font>"
      ],
      "metadata": {
        "id": "d3tHPUa-1RqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"yunaseo/google_gemma_qlora_emotion_detection\")"
      ],
      "metadata": {
        "id": "FYA2EjLN1RqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'indianred'>**3. Test Data Prediction** </font>"
      ],
      "metadata": {
        "id": "jDB1tSeG1RqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = trainer.predict(tokenized_test_dataset[\"train\"])"
      ],
      "metadata": {
        "id": "bL2rBVWK1RqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing for multi-label classification\n",
        "threshold = 0.5  # Example threshold\n",
        "predicted_labels = (predictions.predictions > threshold).astype(int)\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels = [[label for label, binary in zip(labels, binary_labels) if binary] for binary_labels in predicted_labels]\n",
        "\n",
        "# Print or use the predicted labels\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "id": "5NhRCWr21RqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels[0]"
      ],
      "metadata": {
        "id": "xWswAYMZ1RqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "A--JinqL1RqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting data from the 'train' split of test_data\n",
        "test_data_train = test_data['train']\n",
        "tweet_ids = test_data_train['ID']\n",
        "num_tweets = len(tweet_ids)"
      ],
      "metadata": {
        "id": "ixVsGW0v1RqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing an empty dictionary to store the predicted labels\n",
        "predicted_labels_dict = {category: [0] * num_tweets for category in ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']}\n",
        "\n",
        "# Iterate over each row of predicted labels and update the dictionary\n",
        "for i, labels in enumerate(predicted_labels):\n",
        "    for label in labels:\n",
        "        predicted_labels_dict[label][i] = 1"
      ],
      "metadata": {
        "id": "lLzuepEJ1RqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(predicted_labels_dict)\n",
        "\n",
        "# Insert tweet IDs as the first column\n",
        "df.insert(0, 'ID', tweet_ids)"
      ],
      "metadata": {
        "id": "1M1rIpFh1RqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "m98iDdlv1RqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('gemma_qlora_predicted.csv', index=False)"
      ],
      "metadata": {
        "id": "D2uh3ush1RqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('gemma_qlora_predicted.csv')"
      ],
      "metadata": {
        "id": "zan-K3mY1RqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vChv7QSJGjwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <Font color = 'skyblue'>**Task 2. MTEB(BERT) with QLoRA** </font>"
      ],
      "metadata": {
        "id": "U6_vv1dNyGcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'indianred'>**1. Load pre-trained Tokenizer** </font>"
      ],
      "metadata": {
        "id": "cWZRm_DyyGcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distilroberta-base\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "rNh5f0tzyGcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(batch):\n",
        "    return tokenizer(text = batch[\"text\"], truncation=True)"
      ],
      "metadata": {
        "id": "AF67LBGKyGcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset= emotion_data.map(tokenize_fn, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(\n",
        "    ['text']\n",
        ")\n",
        "# tokenized_dataset.set_format(type='torch')"
      ],
      "metadata": {
        "id": "ZTu05sVOyGcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "xshflpOfyGcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_dataset = test_data_text.map(tokenize_fn, batched=True)\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(\n",
        "    ['text']\n",
        ")"
      ],
      "metadata": {
        "id": "REAzdPoJyGcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_dataset"
      ],
      "metadata": {
        "id": "YU27BKlzyGcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color = 'indianred'> **2. Model Training**"
      ],
      "metadata": {
        "id": "gW2kGPR_yGcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **2.1. compute_metrics function** </font>\n"
      ],
      "metadata": {
        "id": "AaSTEQpPyGce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load('accuracy', 'multilabel')\n",
        "f1 = evaluate.load('f1','multilabel')\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # print(logits.shape)\n",
        "    preds = (logits > 0).astype(int)\n",
        "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    f1_micro = f1.compute(predictions=preds, references=labels, average='micro')\n",
        "    f1_macro = f1.compute(predictions=preds, references=labels, average='macro')\n",
        "    return {'f1_micro':f1_micro['f1'],\n",
        "            'f1_macro':f1_macro['f1'],\n",
        "            'accuracy':accuracy['accuracy'],\n",
        "            }"
      ],
      "metadata": {
        "id": "U3BBlDEpyGcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **2.2. Training Arguments** </font>"
      ],
      "metadata": {
        "id": "iHgUu7XMyGck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where model checkpoints will be saved\n",
        "run_name = \"emotions_bert_qlora\"\n",
        "base_folder = Path(basepath)\n",
        "model_folder = base_folder / \"models\"/run_name\n",
        "# Create the directory if it doesn't exist\n",
        "model_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Configure training parameters\n",
        "training_args = TrainingArguments(\n",
        "    # Training-specific configurations\n",
        "    num_train_epochs=10,  # Total number of training epochs\n",
        "    # Number of samples per training batch for each device\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    # auto_find_batch_size=True,\n",
        "    weight_decay=1.0,  # Apply L2 regularization to prevent overfitting\n",
        "    learning_rate=1e-4,  # Step size for the optimizer during training\n",
        "    lr_scheduler_type='linear',\n",
        "    warmup_steps=0,  # Number of warmup steps for the learning rate scheduler\n",
        "    optim='adamw_torch',  # Optimizer,\n",
        "\n",
        "    # Checkpoint saving and model evaluation settings\n",
        "    output_dir=str(model_folder),  # Directory to save model checkpoints\n",
        "    evaluation_strategy='steps',  # Evaluate model at specified step intervals\n",
        "    eval_steps=20,  # Perform evaluation every 10 training steps\n",
        "    save_strategy=\"steps\",  # Save model checkpoint at specified step intervals\n",
        "    save_steps=20,  # Save a model checkpoint every 10 training steps\n",
        "    load_best_model_at_end=True,  # Reload the best model at the end of training\n",
        "    save_total_limit=2,  # Retain only the best and the most recent model checkpoints\n",
        "    # Use 'accuracy' as the metric to determine the best model\n",
        "    metric_for_best_model=\"eval_f1_macro\",\n",
        "    greater_is_better=True,  # A model is 'better' if its accuracy is higher\n",
        "\n",
        "\n",
        "    # Experiment logging configurations (commented out in this example)\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=20,\n",
        "    report_to='wandb',  # Log metrics and results to Weights & Biases platform\n",
        "    run_name=run_name,  # Experiment name for Weights & Biases\n",
        "\n",
        "    fp16=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "EvUH06vpyGcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.3. Specify Model** <font/>"
      ],
      "metadata": {
        "id": "FPS6D1bZyGcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  llm_int8_skip_modules = ['score'],\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_use_double_quant=True,\n",
        "  bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "yTtaW917yGcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
        "                                                           num_labels=11,\n",
        "                                                           problem_type=\"multi_label_classification\" )\n",
        "\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "id2label= {id_: label_ for id_, label_ in enumerate(labels)}\n",
        "label2id = {label_: id_ for id_, label_ in enumerate(labels)}\n",
        "config.id2label = id2label\n",
        "config.label2id = label2id\n",
        "model.config = config\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "# model"
      ],
      "metadata": {
        "id": "43GOe--ByGcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "sazPeJ2tyGcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "cY_TlqGQyGcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUF7RX9dyGc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.4. QLoRA Setup** <font/>"
      ],
      "metadata": {
        "id": "ZylkEYQEyGc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import (\n",
        "    TaskType,\n",
        "    LoraConfig,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        ")"
      ],
      "metadata": {
        "id": "hVX39jV7yGc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "model_modules = str(model.modules)\n",
        "pattern = r'\\((\\w+)\\): Linear'\n",
        "linear_layer_names = re.findall(pattern, model_modules)\n",
        "\n",
        "names = []\n",
        "# Print the names of the Linear layers\n",
        "for name in linear_layer_names:\n",
        "    names.append(name)\n",
        "target_modules = list(set(names))\n",
        "target_modules"
      ],
      "metadata": {
        "id": "xNoyCw4XyGc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=128,\n",
        "    lora_alpha=256,\n",
        "    lora_dropout=0.01,\n",
        "    bias=\"lora_only\",\n",
        "    modules_to_save = ['classifier'],\n",
        "    target_modules = ['dense', 'query', 'value', 'key'])\n",
        "bert_model = get_peft_model(model, bert_config )\n",
        "bert_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "FalU_xuLyGc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_config.target_modules"
      ],
      "metadata": {
        "id": "8Ws934m4yGdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model"
      ],
      "metadata": {
        "id": "RZmKxfr-yGdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "msMAdKGjyGdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.5. Custom Trainer**<font/>"
      ],
      "metadata": {
        "id": "703wflUlyGdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pos_weights(dataset):\n",
        "    # Initialize counters for all labels\n",
        "    num_labels = len(dataset['train']['label'][0])\n",
        "    total_positives = [0] * num_labels\n",
        "    total_negatives = [0] * num_labels\n",
        "\n",
        "    # Count positives and negatives for each label\n",
        "    for label_array in dataset['train']['label']:\n",
        "        for i, label in enumerate(label_array):\n",
        "            if label == 1:\n",
        "                total_positives[i] += 1\n",
        "            else:\n",
        "                total_negatives[i] += 1\n",
        "\n",
        "    # Calculate pos_weight for each label\n",
        "    pos_weight = [total_negatives[i] / max(total_positives[i], 1) for i in range(num_labels)]\n",
        "    return torch.tensor(pos_weight)\n",
        "\n",
        "# Calculate the pos_weight using the training set\n",
        "pos_weights = calculate_pos_weights(emotion_data)"
      ],
      "metadata": {
        "id": "5g5AvkaMyGdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights"
      ],
      "metadata": {
        "id": "3tmN9ZBmyGdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights= torch.tensor([2., 3., 2., 2., 2., 3., 2., 3., 2., 4., 4.])"
      ],
      "metadata": {
        "id": "LRLEv2QTyGdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights"
      ],
      "metadata": {
        "id": "0QqnW0ovyGdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # print(inputs)\n",
        "        # Extract labels and remove them from inputs\n",
        "        labels = inputs.pop(\"labels\").float()  # Ensure labels are float for BCE loss\n",
        "        # print(labels)\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        # Compute custom loss (BCEWithLogitsLoss is suitable for multi-label)\n",
        "        # pos_weight can be used to handle class imbalance\n",
        "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weights.to(device))\n",
        "        # Reshape labels to match logits dimensions\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "pvbmuoe7yGdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"valid\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "OjJQ0lC3yGdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='indianred'>**2.5 Setup WanDB**<font/>"
      ],
      "metadata": {
        "id": "ZUIpj-ccyGdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "%env WANDB_PROJECT = emotions_kaggle_S2024"
      ],
      "metadata": {
        "id": "ahjBwd9myGdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'indianred'> **2.6 Start Training** <font/>"
      ],
      "metadata": {
        "id": "AezUI3QjyGdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()  # start training"
      ],
      "metadata": {
        "id": "uN_kegi5yGdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='indianred'> **2.7 Validation**<font/>"
      ],
      "metadata": {
        "id": "G43rOJ_3yGdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(tokenized_dataset[\"valid\"])"
      ],
      "metadata": {
        "id": "28lm0OjbyGdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results"
      ],
      "metadata": {
        "id": "TeRppeTNyGdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.log({\"eval_accuracy\": eval_results[\"eval_accuracy\"], \"eval_loss\": eval_results[\"eval_loss\"],\n",
        "\"eval_f1_micro\": eval_results[\"eval_f1_micro\"], \"eval_f1_macro\": eval_results[\"eval_f1_macro\"]})"
      ],
      "metadata": {
        "id": "d80SO4z0yGdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, let us check the best checkpoint\n",
        "# We need this for Inference\n",
        "best_model_checkpoint_step = trainer.state.best_model_checkpoint.split('-')[-1]\n",
        "print(f\"The best model was saved at step {best_model_checkpoint_step}.\")"
      ],
      "metadata": {
        "id": "Him-wL0AyGds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **Check Confusion Matrix**</font>"
      ],
      "metadata": {
        "id": "z41LvQMEyGdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trainer to generate predictions on the tokenized validation dataset.\n",
        "# The resulting object, valid_output, will contain the model's logits (raw prediction scores) for each input in the validation set.\n",
        "valid_output = trainer.predict(tokenized_dataset[\"valid\"])"
      ],
      "metadata": {
        "id": "CeBZLrdbyGdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_valid = (valid_output.predictions > 0).astype(int)\n",
        "labels_valid = valid_output.label_ids.astype(int)"
      ],
      "metadata": {
        "id": "lcbdC_biyGeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = labels_valid\n",
        "y_pred = predictions_valid\n",
        "class_names = labels\n",
        "\n",
        "mcm = multilabel_confusion_matrix(y_true, y_pred,)\n",
        "\n",
        "# 1. Individual Heatmaps\n",
        "for idx, matrix in enumerate(mcm):\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues',\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['True Negative', 'True Positive'])\n",
        "    plt.title(f'Confusion Matrix for {class_names[idx]}')\n",
        "    plt.show()\n",
        "\n",
        "# 2. Aggregate Metrics Heatmap\n",
        "precision_per_class = precision_score(y_true, y_pred, average=None)\n",
        "recall_per_class = recall_score(y_true, y_pred, average=None)\n",
        "f1_per_class = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Precision': precision_per_class,\n",
        "    'Recall': recall_per_class,\n",
        "    'F1-Score': f1_per_class\n",
        "}, index=class_names)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(metrics_df, annot=True, cmap='Blues')\n",
        "# plt.title('Metrics for each class')\n",
        "# plt.show()\n",
        "\n",
        "ax = sns.heatmap(metrics_df, annot=True, cmap='Blues')\n",
        "plt.title('Metrics for each class')\n",
        "plt.tight_layout()  # Adjust layout to not cut off edges\n",
        "\n",
        "# Log the heatmap to wandb\n",
        "wandb.log({\"Metrics Heatmap\": wandb.Image(ax.get_figure())})\n",
        "plt.show()\n",
        "\n",
        "# 3. Histogram of Metrics\n",
        "metrics_df.plot(kind='bar', figsize=(12, 7))\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision, Recall, and F1-Score for Each Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ocTKekAHyGev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "X-yK9AF0yGew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **Save the model on HuggingFace**</font>"
      ],
      "metadata": {
        "id": "x0MwW6GayGex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.push_to_hub(\"yunase/Bert_QLoRA_emotion_detection\")"
      ],
      "metadata": {
        "id": "A_8adri1yGex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'indianred'>**3. Test Data Prediction** </font>"
      ],
      "metadata": {
        "id": "sEXfmu27yGex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = trainer.predict(tokenized_test_dataset[\"train\"])"
      ],
      "metadata": {
        "id": "2zacDBtNyGey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing for multi-label classification\n",
        "threshold = 0.5  # Example threshold\n",
        "predicted_labels = (predictions.predictions > threshold).astype(int)\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels = [[label for label, binary in zip(labels, binary_labels) if binary] for binary_labels in predicted_labels]\n",
        "\n",
        "# Print or use the predicted labels\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "id": "URM4cYftyGez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels[0]"
      ],
      "metadata": {
        "id": "u93p54xQyGez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "3V2SVQJIyGe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting data from the 'train' split of test_data\n",
        "test_data_train = test_data['train']\n",
        "tweet_ids = test_data_train['ID']\n",
        "num_tweets = len(tweet_ids)"
      ],
      "metadata": {
        "id": "e3cG5ckZyGe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing an empty dictionary to store the predicted labels\n",
        "predicted_labels_dict = {category: [0] * num_tweets for category in ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']}\n",
        "\n",
        "# Iterate over each row of predicted labels and update the dictionary\n",
        "for i, labels in enumerate(predicted_labels):\n",
        "    for label in labels:\n",
        "        predicted_labels_dict[label][i] = 1"
      ],
      "metadata": {
        "id": "ZiPOaWmoyGe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(predicted_labels_dict)\n",
        "\n",
        "# Insert tweet IDs as the first column\n",
        "df.insert(0, 'ID', tweet_ids)"
      ],
      "metadata": {
        "id": "JnTE-ABeyGe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "J0goCaiJyGe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('bert_qlora_predicted.csv', index=False)"
      ],
      "metadata": {
        "id": "dy0MThrxyGe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('bert_qlora_predicted.csv')"
      ],
      "metadata": {
        "id": "A16rvbLPyGe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <Font color = 'skyblue'>**Task 3 (optional) BERT** </font>"
      ],
      "metadata": {
        "id": "v9PSiOh6TJKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'indianred'>**1. Load pre-trained Tokenizer** </font>"
      ],
      "metadata": {
        "id": "79VeyOlyTJK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distilroberta-base\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "vu9eNrO6TJK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(batch):\n",
        "    return tokenizer(text = batch[\"text\"], truncation=True)"
      ],
      "metadata": {
        "id": "AuvmkD3cTJK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset= emotion_data.map(tokenize_fn, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(\n",
        "    ['text']\n",
        ")\n",
        "# tokenized_dataset.set_format(type='torch')"
      ],
      "metadata": {
        "id": "RudI3kQUTJK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "9ToMnAEpTJK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_dataset = test_data_text.map(tokenize_fn, batched=True)\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(\n",
        "    ['text']\n",
        ")"
      ],
      "metadata": {
        "id": "jwhZQrWJTJK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_dataset"
      ],
      "metadata": {
        "id": "YzVMlWEATJK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color = 'indianred'> **2. Model Training**"
      ],
      "metadata": {
        "id": "LcT5SwUrTJK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **2.1. compute_metrics function** </font>\n"
      ],
      "metadata": {
        "id": "tE8inGEJTJK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load('accuracy', 'multilabel')\n",
        "f1 = evaluate.load('f1','multilabel')\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # print(logits.shape)\n",
        "    preds = (logits > 0).astype(int)\n",
        "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    f1_micro = f1.compute(predictions=preds, references=labels, average='micro')\n",
        "    f1_macro = f1.compute(predictions=preds, references=labels, average='macro')\n",
        "    return {'f1_micro':f1_micro['f1'],\n",
        "            'f1_macro':f1_macro['f1'],\n",
        "            'accuracy':accuracy['accuracy'],\n",
        "            }"
      ],
      "metadata": {
        "id": "-hMfSp02TJLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **2.2. Training Arguments** </font>"
      ],
      "metadata": {
        "id": "5ZZl-TALTJLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where model checkpoints will be saved\n",
        "run_name = \"emotions_bert\"\n",
        "base_folder = Path(basepath)\n",
        "model_folder = base_folder / \"models\"/run_name\n",
        "# Create the directory if it doesn't exist\n",
        "model_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Configure training parameters\n",
        "training_args = TrainingArguments(\n",
        "    # Training-specific configurations\n",
        "    num_train_epochs=10,  # Total number of training epochs\n",
        "    # Number of samples per training batch for each device\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    # auto_find_batch_size=True,\n",
        "    weight_decay=1.0,  # Apply L2 regularization to prevent overfitting\n",
        "    learning_rate=1e-4,  # Step size for the optimizer during training\n",
        "    lr_scheduler_type='linear',\n",
        "    warmup_steps=0,  # Number of warmup steps for the learning rate scheduler\n",
        "    optim='adamw_torch',  # Optimizer,\n",
        "\n",
        "    # Checkpoint saving and model evaluation settings\n",
        "    output_dir=str(model_folder),  # Directory to save model checkpoints\n",
        "    evaluation_strategy='steps',  # Evaluate model at specified step intervals\n",
        "    eval_steps=20,  # Perform evaluation every 10 training steps\n",
        "    save_strategy=\"steps\",  # Save model checkpoint at specified step intervals\n",
        "    save_steps=20,  # Save a model checkpoint every 10 training steps\n",
        "    load_best_model_at_end=True,  # Reload the best model at the end of training\n",
        "    save_total_limit=2,  # Retain only the best and the most recent model checkpoints\n",
        "    # Use 'accuracy' as the metric to determine the best model\n",
        "    metric_for_best_model=\"eval_f1_macro\",\n",
        "    greater_is_better=True,  # A model is 'better' if its accuracy is higher\n",
        "\n",
        "\n",
        "    # Experiment logging configurations (commented out in this example)\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=20,\n",
        "    report_to='wandb',  # Log metrics and results to Weights & Biases platform\n",
        "    run_name=run_name,  # Experiment name for Weights & Biases\n",
        "\n",
        "    fp16=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "BceEN8K2TJLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.3. Specify Model** <font/>"
      ],
      "metadata": {
        "id": "qcLzsARoTJLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
        "                                                           num_labels=11,\n",
        "                                                           problem_type=\"multi_label_classification\" )\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "id2label= {id_: label_ for id_, label_ in enumerate(labels)}\n",
        "label2id = {label_: id_ for id_, label_ in enumerate(labels)}\n",
        "config.id2label = id2label\n",
        "config.label2id = label2id\n",
        "model.config = config\n",
        "\n",
        "# model"
      ],
      "metadata": {
        "id": "gR8Q5JNfTJLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "JaogcHsfTJLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "E1qUXKvXUW2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Font color='indianred'> **2.4. Custom Trainer**<font/>"
      ],
      "metadata": {
        "id": "eakfyg23TJLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pos_weights(dataset):\n",
        "    # Initialize counters for all labels\n",
        "    num_labels = len(dataset['train']['label'][0])\n",
        "    total_positives = [0] * num_labels\n",
        "    total_negatives = [0] * num_labels\n",
        "\n",
        "    # Count positives and negatives for each label\n",
        "    for label_array in dataset['train']['label']:\n",
        "        for i, label in enumerate(label_array):\n",
        "            if label == 1:\n",
        "                total_positives[i] += 1\n",
        "            else:\n",
        "                total_negatives[i] += 1\n",
        "\n",
        "    # Calculate pos_weight for each label\n",
        "    pos_weight = [total_negatives[i] / max(total_positives[i], 1) for i in range(num_labels)]\n",
        "    return torch.tensor(pos_weight)\n",
        "\n",
        "# Calculate the pos_weight using the training set\n",
        "pos_weights = calculate_pos_weights(emotion_data)"
      ],
      "metadata": {
        "id": "lPHvn68kTJLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights"
      ],
      "metadata": {
        "id": "s2AcSiZ9TJLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights= torch.tensor([2., 3., 2., 2., 2., 3., 2., 3., 2., 4., 4.])"
      ],
      "metadata": {
        "id": "UiBDvXb_TJLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights"
      ],
      "metadata": {
        "id": "o8KBvYXKTJLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # print(inputs)\n",
        "        # Extract labels and remove them from inputs\n",
        "        labels = inputs.pop(\"labels\").float()  # Ensure labels are float for BCE loss\n",
        "        # print(labels)\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        # Compute custom loss (BCEWithLogitsLoss is suitable for multi-label)\n",
        "        # pos_weight can be used to handle class imbalance\n",
        "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weights.to(device))\n",
        "        # Reshape labels to match logits dimensions\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "sOULTW3aTJLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"valid\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "jAvsdDoVTJLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='indianred'>**2.5 Setup WanDB**<font/>"
      ],
      "metadata": {
        "id": "aMjP_hYcTJLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "%env WANDB_PROJECT = emotions_kaggle_S2024"
      ],
      "metadata": {
        "id": "1qiNkPqeTJLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'indianred'> **2.6 Start Training** <font/>"
      ],
      "metadata": {
        "id": "N1SujyhNTJLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()  # start training"
      ],
      "metadata": {
        "id": "AUodn_GSTJLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='indianred'> **2.7 Validation**<font/>"
      ],
      "metadata": {
        "id": "UpmBK7kyTJLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(tokenized_dataset[\"valid\"])"
      ],
      "metadata": {
        "id": "_kFRVan2TJLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results"
      ],
      "metadata": {
        "id": "iXQpa22sTJLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.log({\"eval_accuracy\": eval_results[\"eval_accuracy\"], \"eval_loss\": eval_results[\"eval_loss\"],\n",
        "\"eval_f1_micro\": eval_results[\"eval_f1_micro\"], \"eval_f1_macro\": eval_results[\"eval_f1_macro\"]})"
      ],
      "metadata": {
        "id": "QaNz87lSTJLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, let us check the best checkpoint\n",
        "# We need this for Inference\n",
        "best_model_checkpoint_step = trainer.state.best_model_checkpoint.split('-')[-1]\n",
        "print(f\"The best model was saved at step {best_model_checkpoint_step}.\")"
      ],
      "metadata": {
        "id": "EUohl5IGTJLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **Check Confusion Matrix**</font>"
      ],
      "metadata": {
        "id": "BGW35I6QTJLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trainer to generate predictions on the tokenized validation dataset.\n",
        "# The resulting object, valid_output, will contain the model's logits (raw prediction scores) for each input in the validation set.\n",
        "valid_output = trainer.predict(tokenized_dataset[\"valid\"])"
      ],
      "metadata": {
        "id": "FuJ0egBOTJLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_valid = (valid_output.predictions > 0).astype(int)\n",
        "labels_valid = valid_output.label_ids.astype(int)"
      ],
      "metadata": {
        "id": "bwcLEdBLTJLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = labels_valid\n",
        "y_pred = predictions_valid\n",
        "class_names = labels\n",
        "\n",
        "mcm = multilabel_confusion_matrix(y_true, y_pred,)\n",
        "\n",
        "# 1. Individual Heatmaps\n",
        "for idx, matrix in enumerate(mcm):\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues',\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['True Negative', 'True Positive'])\n",
        "    plt.title(f'Confusion Matrix for {class_names[idx]}')\n",
        "    plt.show()\n",
        "\n",
        "# 2. Aggregate Metrics Heatmap\n",
        "precision_per_class = precision_score(y_true, y_pred, average=None)\n",
        "recall_per_class = recall_score(y_true, y_pred, average=None)\n",
        "f1_per_class = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Precision': precision_per_class,\n",
        "    'Recall': recall_per_class,\n",
        "    'F1-Score': f1_per_class\n",
        "}, index=class_names)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(metrics_df, annot=True, cmap='Blues')\n",
        "# plt.title('Metrics for each class')\n",
        "# plt.show()\n",
        "\n",
        "ax = sns.heatmap(metrics_df, annot=True, cmap='Blues')\n",
        "plt.title('Metrics for each class')\n",
        "plt.tight_layout()  # Adjust layout to not cut off edges\n",
        "\n",
        "# Log the heatmap to wandb\n",
        "wandb.log({\"Metrics Heatmap\": wandb.Image(ax.get_figure())})\n",
        "plt.show()\n",
        "\n",
        "# 3. Histogram of Metrics\n",
        "metrics_df.plot(kind='bar', figsize=(12, 7))\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision, Recall, and F1-Score for Each Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9OA5irscTJLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "9GzGgH0-TJLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'indianred'> **Save the model on HuggingFace**</font>"
      ],
      "metadata": {
        "id": "eSzs-nbiTJLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"yunase/Bert_emotion_detection\")"
      ],
      "metadata": {
        "id": "6gHODGK4TJLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'indianred'>**3. Test Data Prediction** </font>"
      ],
      "metadata": {
        "id": "puIfsNphTJLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = trainer.predict(tokenized_test_dataset[\"train\"])"
      ],
      "metadata": {
        "id": "bTihD9fhTJLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing for multi-label classification\n",
        "threshold = 0.5  # Example threshold\n",
        "predicted_labels = (predictions.predictions > threshold).astype(int)\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels = [[label for label, binary in zip(labels, binary_labels) if binary] for binary_labels in predicted_labels]\n",
        "\n",
        "# Print or use the predicted labels\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "id": "PrqyVFQCTJLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels[0]"
      ],
      "metadata": {
        "id": "7jXE6QsxTJLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "gNFMfj0fTJLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting data from the 'train' split of test_data\n",
        "test_data_train = test_data['train']\n",
        "tweet_ids = test_data_train['ID']\n",
        "num_tweets = len(tweet_ids)"
      ],
      "metadata": {
        "id": "dqYH8teITJLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing an empty dictionary to store the predicted labels\n",
        "predicted_labels_dict = {category: [0] * num_tweets for category in ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']}\n",
        "\n",
        "# Iterate over each row of predicted labels and update the dictionary\n",
        "for i, labels in enumerate(predicted_labels):\n",
        "    for label in labels:\n",
        "        predicted_labels_dict[label][i] = 1"
      ],
      "metadata": {
        "id": "hXqiO1EtTJLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(predicted_labels_dict)\n",
        "\n",
        "# Insert tweet IDs as the first column\n",
        "df.insert(0, 'ID', tweet_ids)"
      ],
      "metadata": {
        "id": "LbiYi4GwTJLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "M7KTV6lwTJLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('bert_predicted.csv', index=False)"
      ],
      "metadata": {
        "id": "P1AZqAnNTJLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('bert_predicted.csv')"
      ],
      "metadata": {
        "id": "HFRMaZlpTJLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "m4GxqANW1RqK",
        "nIbNImJl1RqM",
        "LkoSDk6T1RqM",
        "QAMwlTDU1RqN",
        "6zx0OqyJ1RqN",
        "pj-iouwP1RqP",
        "Zsal8wFl1RqR",
        "lHMIl7cw1RqT",
        "jDB1tSeG1RqU"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "a75929e85776ec4eb9d0646d55dce2d9636f6a302fc78e3e6babaf08df8451b4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}